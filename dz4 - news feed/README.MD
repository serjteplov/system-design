
# Задание
Вам предстоит спроектировать систему хранения данных для высоконагруженного веб-приложения, которая сочетает реляционные или NoSQL базы данных, кэширование и Content Delivery Network (CDN). Задача заключается в создании архитектуры, которая обеспечивает высокую производительность, масштабируемость и надёжность при одновременной оптимизации затрат.

# Архитектура системы
В качестве проектируемой системы выберем News Feed в социальной сети.
Разработана следующая архитектура:
![System-design-dz1-ДЗ-4_1.jpg](System-design-dz1-%D0%94%D0%97-4_1.jpg)

# 1. Выбор и проектирование базы данных:
* Есть 3 микросервиса в которых хранятся данные: user service(SQL), publication service(NoSQL), feed service(NoSQL)
* Обоснование SQL: user service используется для хранения данных о пользователе. Используем SQL хранилище, потому что здесь важна консистентность и транзакционность данных. Консистентность важна с точки зрения настроек, которые применяет пользователь в своем профиле. Настройки ленты, личные данные должны всегда находится в согласованном состоянии в профиле, в публикации, в комментариях.
* Обоснование NoSQL: publication service, feed service используют структуры данных, подверженные частым изменениям, например добавлению нового атрибута. Также, в связи с гигантским количеством публикаций эти данные нужно масштабировать, сохраняя при этом низкое лэйтенси на чтение. При этом строгая консистентность не важна, можем пожертвовать актуальностью данных в моменте и согласиться на eventual consistency
* Ниже представлена схема БД для MongoDB
* Коллекция Publications:

| Поле             | Тип данных   | Описание                               | Индекс           |
|------------------|--------------|----------------------------------------|------------------|
| `_id`            | String(UUID) | Уникальный идентификатор публикации    | Primary Key      |
| `authorId`       | String       | Идентификатор автора публикации        | Index (compound) |
| `content`        | String       | Текст публикации                       |                  |
| `latestComments` | Array        | Массив последних комментариев (N штук) |                  |
| `createdAt`      | DateTime     | Дата и время создания публикации       | Index (compound) |

```json
{
    "_id": "pub_123",
    "authorId": "user_1",
    "content": "Моя публикация",
    "latestComments": [
        {
        "commentId": "comm_1001",
        "text": "Последний комментарий!",
        "createdAt": "2025-03-01T14:00:00Z"
        }
    ],
    "createdAt": "2025-03-01T13:00:00Z"
}
```

* Коллекция Comments:

| Поле            | Тип данных   | Описание                               | Индекс                         |
|-----------------|--------------|----------------------------------------|--------------------------------|
| `_id`           | String(UUID) | Уникальный идентификатор комментария   | Primary Key                    |
| `publicationId` | String(UUID) | ID публикации (связь с публикацией)    | Index (compound с `createdAt`) |
| `authorId`      | String       | Идентификатор автора комментария       | Index                          |
| `text`          | String       | Текст комментария                      |                                |
| `createdAt`     | DateTime     | Дата и время создания комментария      | Index (compound)               |
```json
{
    "_id": "comm_1",
    "publicationId": "pub_123",
    "authorId": "user_2",
    "text": "Комментарий",
    "createdAt": "2025-03-01T13:05:00Z"
}
```

* Коллекция Feed:

| Поле            | Тип данных   | Описание                                    | Индекс                         |
|-----------------|--------------|---------------------------------------------|--------------------------------|
| `_id`           | String(UUID) | Уникальный идентификатор комментария        | Primary Key                    |
| `publicationId` | String(UUID) | ID публикации (связь с публикацией)         |                                |
| `userId`        | String       | ID пользователя, которому принадлежит лента | Index (compound с `createdAt`) |
| `createdAt`     | DateTime     | Дата и время создания комментария           | Index (compound)               |
```json
{
    "_id": "pub_1",
    "publicationId": "pub_123",
    "userId": "user_2",
    "createdAt": "2025-03-01T13:05:00Z"
}
```
* Описание связи: коллекция `Comments` связана с `Publications` за счет аттрибута `publicationId` в коллекции `Comments`
* Комментарии хранятся отдельно от публикаций для избежания множественных блокировок на запись для популярных постов. Для таких постов за секунду может создаваться тысячи комментариев. Вместо этого используем секцию `latestComments` где будут храниться последние комментарии. Заполнения этой секции будет осуществляться в асинхронном режиме раз в одну-две секунды. Для этого в publication-service будет создан слушатель топика кафки, который будет делать выборку из `Comments` и вставлять аггрегированные данные в `latestComments`.
* Стратегии репликации и шардирования:
* Для поиска комментариев по `publicationId` создадим compaund индекс на поля `publicationId`+`createdAt` чтобы быстро находить последние комментарии в коллекции `Comments` по id публикации. Также компаунд индекс понадобится для шардирования коллекции комментариев по полю `publicationId`+`createdAt`+random digit. Данная модель шардирования позволит равномерно разбить данные по шардам и избежать skew. Однако проигрываем в скорости поиска, потому что для поиска по комментам придется бегать по всем шардам.
* Для поиска `Publications` по автору создадим compound индекс `authorId`+`createdAt`, он же понадобится для фильтрации публикаций по дате создания. Также, compound индекс понадобится для шардирования коллекции публикаций по полю `authorId`+`createdAt`. Данная модель шардирования позволит разбить данные по шардам, не теряя в скорости поиска публикаций. Однако при данном ключе шардирования возможны skew если количество активных авторов небольшое. Если есть 12 шардов и надо делить данные по ключу `authorId`+`createdAt`, то в одной шарде могут быть данные разных авторов за разное время, т.к. монга шардирует данные по диапазонам. При необходимости можно добавить дополнительное количество шард.
* Проблема свободного места на шардах решается архивированием данных. Коротко - джоба ежедневно вычисляет записи, подлежащие архивированию, и в асинхронном режиме отправляет данные в холодное S3 хранилище.
* Стратегия репликация стандартная. Каждая шарда MongoDB это replica set, состоящий из мастер-ноды и двух реплик, разнесённых по разным серверам. На каждую шарду используется 3 физических сервера (по 1 на реплику). При 12 шардах, потребуется 36 серверов.

# 2. Проектирование системы кэширования:
Для минимизации рисков неконсистентности кэша используется:
1. Write-through схема, при которой обновления в БД синхронно пишутся в кэш и в БД
2. Ручная инвалидация данных в кэше, например если пользователь не заходил 10мин, то инвалидируем кэш настроек.
3. Обновление данных в кэше по событию кафки
4. Небольшой TTL, автоматическая инвалидация через небольшой промежуток времени

### Архитектура кэша
| Сервис             | Описание кэширования                                                                                                                                                                                                                           | Стратегия кэширования                                                                                                                                                                                                                                                    |
|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **user-service**   | Кэшируется список настроек ленты пользователя. <br> Кэшируем только активных пользователей. Если пользователь не заходил в ленту 10 минут, очищаем кэш (Session-Based кэширование).                                                          | - **Добавление:** при первом запросе (Read-Through) <br> - **Обновление:** при обновлении данных инвалидируем кэш <br> - **Сброс:** по TTL=2 часа или если пользователь не заходил в ленту 10 минут <br> - **Вытеснение:** LFU |
| **publication-service** | Кэшируются публикации пользователей. <br> Для 10 млн активных пользователей кэшировать все публикации невозможно, поэтому: <br> 1. Кэшируем посты блогеров с >1000 подписчиков (Redis, TTL=2 суток) <br> 2. Кэшируем посты с большим числом просмотров (Redis, TTL=12 часов) <br> 3. Прогреваем кэш топ-100 популярными публикациями перед стартом сервиса | - **Добавление:** <br>   - Для блогеров: Write-Through <br>   - Для постов с >1k просмотров: асинхронно через Kafka <br> - **Обновление:** при обновлении популярной публикации обновляем кэш <br> - **Сброс:** по TTL или ручная инвалидация <br> - **Вытеснение:** LFU (блогеры), LRU (посты с просмотрами) |
| **friend-service** | Кэшируется список подписчиков и друзей пользователя.                                                                                                                                                                                          | - **Добавление:** Write-Through для пользователей с >1000 подписчиков <br> - **Обновление:** синхронно обновляем кэш и БД при изменении числа подписчиков <br> - **Сброс:** TTL=6 часов или вручную <br> - **Вытеснение:** LFU |
| **feed-service**   | Кэшируется список пропущенных публикаций в ленте пользователя. <br> Кэш нужен, чтобы не терять сообщения между первым запросом ленты и установлением WebSocket-соединения. <br> Запись в кэш при получении публикации из Kafka. Хранятся последние 15 публикаций (LTRIM). <br> Если пользователь неактивен >30 минут, запись в кэш прекращается, возобновляется при первом запросе ленты. | - **Добавление:** по событию из Kafka <br> - **Обновление:** LTRIM <br> - **Сброс:** TTL=5 минут <br> - **Вытеснение:** LRU                                                                                                                                             |

# 3. Интеграция CDN:
В CDN будем хранить медиа файлы и статику.
### Концептуальная схема работы:
1. В MongoDB publication-service хранятся джейсоны, в которых есть аттрибут "image": "https://cdn-load-balancer.feed.com/image1"
2. Пользователь из отдаленного региона открывает ленту, фронт получает джейсон с этим аттрибутом "image"
3. Фронт идет по адресу балансировщика https://cdn-load-balancer.feed.com/image1 который, используя DNS балансировку, резолвит этот урл на IP географически ближайшего edge server
4. Если картинка есть на edge сервере, то она моментально возвращается пользователю, если её нет, то CDN идет в origin сервер (S3 minIo на своём железе) и добавляет её на данный edge сервер
5. Если автор удалил или изменил картинку в своём посте, поменялся json публикации, а именно аттрибут "image". Пользователь из отдаленного региона, обновив страницу, подгрузит обновленный json, дальше выполнятся пункты 3 и 4.
6. Для версионирования файлов, будем писать версию в самом урле "image": "https://cdn-load-balancer.feed.com/image1?v=132878"
![System-design-dz1-ДЗ-4.jpg](System-design-dz1-%D0%94%D0%97-4.jpg)

# 4. Работа с требованиями системы:
### Опишите, как ваша архитектура учитывает требования к согласованности, доступности и производительности.
При запросе ленты пользователем, происходит запрос ленты из Mongo сервиса feed-service.
После этого и ДО момента установления websocket соединения между фронтом и бэком, feed-service сохраняет в кэш все новые посты. Это обеспечивает **консистентность** ленты пользователя на фронте и бэке. Также, при формировании ленты перед отдачей ее на фронт, feed-service ходит по кэшам смежных сервисов, эти кэши обновляются или инвалидируются каждый раз, когда меняются данные в БД. Для обновления кэшей используется кафка, джобы, write-through, что уменьшает **latency** получения **консистентых** данных.
Для публикаций топовых авторов создан отдельный кэш, что окажет положительное влияние на производительность, т.к. значения будут реже вытесняться. 
Есть две базы данных, которые достигают огромных размеров - это БД публикаций и БД ленты пользователей. Шардирование этих баз добавит очков к **доступности** и **производительности**.
База данных feed-service, которая хранит ленты пользователей, хранит данные 3 дня, всё что старше - удаляется навсегда, а если пользователь запросит исторические данные по своей ленте, скажем неделю назад, ему будет скомпанована интеллектуальная лента из рекомендованных постов с помощью AI сервиса. AI сервис будет получать данные из аналитической БД GreenPlum, данные в которую будут попадать по CQRS из основной базы. Такой подход улучшает производительность и экономит ресурсы, т.к. в основной БД данных храним не много.

### Приведите примеры, как система справляется с пиковыми нагрузками (например, большой наплыв пользователей).
**Пример1.** Допустим миллион пользователей в news feed в один день написали по 20 публикаций.
Данные посты легли в Mongo сервиса публикаций и равномерно распределились по шардам MongoDB хранилища публикаций.
Далее, через топик Kafka эти публикации доставятся в feed-service. 
Из-за огромного числа публикаций, возможны задержки в доставке сообщений на feed-service.
Сервис feed-service будет автоматически скалирован в Кубернетесе чтобы подстроится под высокую нагрузку.
При построении ленты feed-service будет ходить в кэш publication-service, но из-за большого наплыва публикаций данные будут часто вытесняться из кэша, создавая тем самым задержку в формировании ленты. 
Итог: увеличится latency первого запроса ленты, возникнут задержки в обновлении ленты по websocket соединению.\

**Пример2.** Допустим миллион пользователей одновременно открыли news feed чтобы посмотреть важную новость.
Первый запрос идет напрямую в feed-service в MongoDB где хранится лента публикаций. Запросы равномерно распределяются по 30 шардам. В коллекции Feed создан compound index на поля userId+createdAt, таким образом запросы отрабатывают очень быстро. Однако, не смотря на разбиение по 30 шардам, из-за большого наплыва одновременных пользователей, запрос к MongoDB будет тормозить, пока наплыв не спадёт. Кэш в таких ситуациях не поможет, т.к. он не выдержит нагрузки и упадет, после чего все запросы устремятся в MongoDB.Защититься от таких ситуаций можно паттернами rate limiter, circuit breaker, добавлением количества шард в кластер. Либо, чтобы не падал Redis, можно использовать кластер Redis Sentinel для обеспечения отказоустойчивости

# 5. Мониторинг и отладка:
### Укажите ключевые метрики для мониторинга базы данных, кэша и CDN.
**Метрики мониторинга БД:**
1. CPU/RAM/Disk space available
4. Connection pool
5. Top10 slow SQL queries
6. TPS (min, max)
7. Latency запросов (p95, p99)
8. Replication lag
9. Connection errors

**Метрики монмторинга кэша:**
1. CPU/RAM/Disk space available
2. Cache Hit/Miss rate
3. Latency get/set (p95, p99)
4. Evictions per second (частота вытеснений)
5. TTL expiration rate

**Метрики CDN:**
1. Время отдачи контента из кэша edge сервера
2. Время записи контента в кэш edge сервера
3. Cache Hit/Miss Rate на edge-серверах
4. Traffic volume
5. HTTP errors (4xx, 5xx)

**Метрики Kafka:**
1. Consumer lag (отставание консьюмеров)
2. Throughput (сообщений/сек)
3. Message processing latency

### Предложите инструменты для мониторинга и выявления узких мест в производительности системы.
1. Prometheus
2. Grafana
3. Metrics in backend services
4. Alerting


