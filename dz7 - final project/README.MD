
# Задание
Проблема заказчика:
Компания планирует запустить новую платформу для обработки пользовательских запросов для вызова такси. Платформа должна быть масштабируемой, отказоустойчивой и соответствовать требованиям безопасности. Система должна выдерживать пиковую нагрузку в 50 000 запросов в секунду и обеспечивать согласованность данных.

# 1. Функциональные требования
* Заявка должна быть отправлена по нажатию на кнопку
* После отправки заявки кнопка должна быть заблокирована
* Возможность отправки двух разных заявок, дедупликация
* Заявки можно отправлять с web, mobile, api
* У заявки должен быть жизненный цикл
* У заявки есть исполнитель, и заявитель
* Заявку можно редактировать после отправки, но только некоторые поля
* Заявка должна запускать бизнес процесс на бэкэнде
* Заявка должна иметь систему оценки
* Должна быть роль администратора, имеющего права изменять любую заявку
* *пользователь должен получать уведомления о смене статуса*
* *история изменений заявки должна сохраняться*
* *просмотр списка заявок, фильтрация, поиск*

# 2. Нефункциональные требования
### Производительность
* в пике 50к заявок без просадки в производительности
* 99p rps 5к
* latency 99p 200ms
* latency high season 1000ms
* *throughput за минуту может быть обработано 100к заявок при средней нагрузке*
### Отказоустойчивость
* система должна функционировать при отказе некритичных узлов
* при авариях должна быть graceful degradation
* RTO=1час RPO=5мин
* *Circuit breaker. Для исключения лавинообразного падения части сервисов применяем circuit breaker*
* *rate limiting*
* *fallback сценарии*
### Безопасность
* должна быть авторизация/аутентификация, OIDC
* пользователь может просматривать только свою заявку
* данные должны передаваться по защищенным протоколам, внутренние взаимодействия по mtls
* *Аудит. Кто и когда изменил заявку*
### Масштабируемость
* должен быть автоскейлинг в high season
* должна быть возможность горизонтального масштабироваться на регионы
* должна быть возможность горизонтального масштабироваться на органический рост
* *stateful stateless компоненты*
* *event driven архитектура*
### Доступность
* система должна оставаться доступна при проблемах в сети или ее узлах, но при этом консистентность на первом месте
* *SLA=99.99%*
* *резервные каналы связи между ЦОД*
### Согласованность данных
* должна быть eventual consistency
* max время несогласованности 3 мин
* *конфликт версий при параллельных обновлениях*
* *где нужна strong consistency*

# 3. Архитектура системы

### Архитектура системы
![Архитектура](https://github.com/serjteplov/system-design/blob/205eaffe1272381838ddc8788ca3f5a688e5cfac/dz7%20-%20final%20project/arch.jpg)

### Деплоймент диаграмма
![Развертывание](https://github.com/serjteplov/system-design/blob/5e495cb6f1ee95aff7ac2c6b11e54f3ea0916b90/dz7%20-%20final%20project/dep.jpg)

### Предложенные решения
1. Сделать 3 кластера в разных географически распределенных регионах страны. Паттерн Георезервирование, геораспределенность. Дает плюсы к производительности и отказоустойчивости. Балансирока между ЦОДами делается на уровне Geo DNS.
2. На входе в каждый ЦОД стоит несколько балансировщиков, способных деражать высокую нагрузку. На балансировщиках настроены health чеки, таким образом если кластер не отвечает, балансировщик об этом быстро узнает и пользовательский запрос будет перенаправлен в другой ЦОД на уровне Geo DNS. Таким образом поддерживается отказоустойчивость на уровне ЦОДов.
3. Для раздачи статики фронта, а также кэширования медиа контента используется CDN
4. При записи тела заявки в БД, одновременно отправляется событие в топик Кафки для запуска дальнейших процессов в стиле event driven. В частности запускается механизм поиска водители и формирования заказа на поездку. Паттерн Transactional outbox.
5. Для обеспечения работы в high season, а также при пиковых нагрузках внедряется шардирование БД. Использование данного паттерна позволяет миновать такие проблемы как множественные блокировки коллекций при записи, вся нагрузка ляжет на одну БД, долгие запросы на чтение, огромный индекс.
6. Для обеспечения работы в high season, а также при пиковых нагрузках используется Kubernetes со встроенными механизмами service discovery, балансировки, автоскейлинга. При возрастании нагрузки, автоматически поднимаются новые поды чтобы держать нагрузку в high season (горизонтальное масштабирование).
7. Для легкого горизонтального масштабирования бэкэнд сервисы разрабатываются по принципу stateless, что позволяет с легкостью добавлять или убирать инстансы по мере необходимости.
8. Перед входом в кластер установлен API Gateway, который служит местом применения кроссфункциональных паттернов, работающих на весь кластер (rate limiter, circuit breaker, authorization, logging)
9. Для решения проблемы актуальности и согласованности шардов на всех ЦОД, используется механизм CDC replication. Специальный сервис читает oplog mongoDB и все изменения отправляет в Кафку. Откуда потом изменения вычитываются и делается upsert в шард в других ЦОДах.
10. Для старых заявок организовано холодное хранение в архивном S3. Раз в сутки в сервисе-перекладчике запускается джоба, в результате которой устаревшие заявки попадают сначала в кафку, а затем в отдельно стоящий сервер хранилища S3. Данные хранятся в виде batch json.
11. Консенсус брокеров кафки, а также нод монги обеспечивается встроенными механизмами, позволяющими автоматически самостоятельно выбирать нового лидера.

# 4. Выбор технологий

tbd

# 5. Реализация нефункциональных требований

tbd

# 6. Риски и компромиссы

tbd
